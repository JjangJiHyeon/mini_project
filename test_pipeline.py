import cv2
import torch
import numpy as np
import time
import threading
import queue
import win32com.client
import pythoncom
import json
import os
from ultralytics import YOLO

# Voskì™€ PyAudioëŠ” ì„ íƒ ì‚¬í•­ìœ¼ë¡œ ì²˜ë¦¬ (ì„¤ì¹˜ í™˜ê²½ ë¬¸ì œ ëŒ€ë¹„)
VOSK_AVAILABLE = False
PYAUDIO_AVAILABLE = False
try:
    from vosk import Model, KaldiRecognizer
    import pyaudio
    VOSK_AVAILABLE = True
    PYAUDIO_AVAILABLE = True
except ImportError:
    pass

# ==========================================
# Optimized MVP Test Pipeline: TTS (ìŒì„± ì•ˆë‚´) ë²„ì „
# ==========================================

class MVPTestPipeline:
    def __init__(self):
        print("ìŒì„± ì§€ì› ëª¨ë“œë¡œ ì „í™˜ ì¤‘... ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ì„¤ì •ê°’
        self.inference_size = (320, 320)
        self.frame_skip = 3
        self.frame_count = 0
        self.K_DEPTH = 3000.0 
        self.running = False  # ì œì–´ìš© í”Œë˜ê·¸

        # ìŒì„± ì•ˆë‚´ ì„¤ì • (ë³¼ë¥¨ ë° ë®¤íŠ¸)
        self.volume = 100  # 0 ~ 100
        self.is_muted = False

        # TTS í ë° ìŠ¤ë ˆë“œ ì´ˆê¸°í™”
        self.speech_queue = queue.Queue()
        self.tts_thread = threading.Thread(target=self._tts_worker, daemon=True)
        self.tts_thread.start()

        # STT (ìŒì„± ì¸ì‹) ì´ˆê¸°í™”
        self.model_path = "model-ko" # í•œêµ­ì–´ ëª¨ë¸ í´ë”ëª…
        self.stt_thread = None
        if VOSK_AVAILABLE and PYAUDIO_AVAILABLE and os.path.exists(self.model_path):
            try:
                self.stt_thread = threading.Thread(target=self._stt_worker, daemon=True)
                self.stt_thread.start()
            except Exception as e:
                print(f"âš ï¸ STT ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            reason = "ëª¨ë¸ í´ë” ì—†ìŒ" if not os.path.exists(self.model_path) else "ë¼ì´ë¸ŒëŸ¬ë¦¬(vosk/pyaudio) ë¯¸ì„¤ì¹˜"
            print(f"âš ï¸ ìŒì„± ëª…ë ¹ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ì›ì¸: {reason})")
        
        # ì‹œì‘ ì•Œë¦¼ (ìŠ¤í”¼ì»¤ í™•ì¸ìš©)
        self.speak("ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ìŒì„± ìƒíƒœ ê´€ë¦¬
        self.announced_objects = {} # {label: last_seen_time}
        self.announce_timeout = 8.0 # 8ì´ˆ ë™ì•ˆ ì•ˆ ë³´ì´ë©´ ì•ˆë‚´ ëª©ë¡ì—ì„œ ì‚­ì œ (ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ë©´ ë§í•¨)

        # ëª¨ë¸ ë¡œë”©
        self.yolo_model = YOLO('yolov8n.pt') 
        self.depth_model_type = "MiDaS_small"
        self.midas = torch.hub.load("intel-isl/MiDaS", self.depth_model_type, trust_repo=True)
        self.device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
        self.midas.to(self.device).eval()
        
        midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms", trust_repo=True)
        self.transform = midas_transforms.small_transform if self.depth_model_type == "MiDaS_small" else midas_transforms.dpt_transform

        self.last_objects = []
        self.last_depth_map = None
        self.last_depth_viz = None
        
        # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš© ë²„í¼
        self.last_web_frame = None
        self.frame_lock = threading.Lock()

        # í•œêµ­ì–´ í´ë˜ìŠ¤ ë§µ
        self.class_names_ko = {
            'person': 'ì‚¬ëŒ', 'bicycle': 'ìì „ê±°', 'car': 'ìë™ì°¨', 'motorcycle': 'ì˜¤í† ë°”ì´',
            'bus': 'ë²„ìŠ¤', 'truck': 'íŠ¸ëŸ­', 'traffic light': 'ì‹ í˜¸ë“±', 'stop sign': 'ì •ì§€ í‘œì§€íŒ',
            'bench': 'ë²¤ì¹˜', 'dog': 'ê°œ', 'cat': 'ê³ ì–‘ì´', 'backpack': 'ë°°ë‚­', 'umbrella': 'ìš°ì‚°',
            'handbag': 'í•¸ë“œë°±', 'tie': 'ë„¥íƒ€ì´', 'suitcase': 'ì—¬í–‰ê°€ë°©', 'sports ball': 'ê³µ',
            'bottle': 'ë³‘', 'wine glass': 'ì™€ì¸ì”', 'cup': 'ì»µ', 'fork': 'í¬í¬', 'knife': 'ì¹¼',
            'spoon': 'ìˆŸê°€ë½', 'bowl': 'ê·¸ë¦‡', 'banana': 'ë°”ë‚˜ë‚˜', 'apple': 'ì‚¬ê³¼', 'sandwich': 'ìƒŒë“œìœ„ì¹˜',
            'orange': 'ì˜¤ë Œì§€', 'broccoli': 'ë¸Œë¡œì½œë¦¬', 'carrot': 'ë‹¹ê·¼', 'hot dog': 'í•«ë„ê·¸', 'pizza': 'í”¼ì',
            'donut': 'ë„ë„›', 'cake': 'ì¼€ì´í¬', 'chair': 'ì˜ì', 'couch': 'ì†ŒíŒŒ', 'potted plant': 'í™”ë¶„',
            'bed': 'ì¹¨ëŒ€', 'dining table': 'ì‹íƒ', 'toilet': 'ë³€ê¸°', 'tv': 'TV', 'laptop': 'ë…¸íŠ¸ë¶',
            'mouse': 'ë§ˆìš°ìŠ¤', 'remote': 'ë¦¬ëª¨ì»¨', 'keyboard': 'í‚¤ë³´ë“œ', 'cell phone': 'í•¸ë“œí°',
            'microwave': 'ì „ìë ˆì¸ì§€', 'oven': 'ì˜¤ë¸', 'í† ìŠ¤í„°': 'í† ìŠ¤í„°', 'sink': 'ì‹±í¬ëŒ€',
            'refrigerator': 'ëƒ‰ì¥ê³ ', 'book': 'ì±…', 'clock': 'ì‹œê³„', 'vase': 'ê½ƒë³‘', 'scissors': 'ê°€ìœ„',
            'teddy bear': 'ê³°ì¸í˜•', 'hair drier': 'í—¤ì–´ë“œë¼ì´ì–´', 'toothbrush': 'ì¹«ì†”'
        }

        # Walking assistance ROI (Center 40%)
        self.roi_x_min = 0.3
        self.roi_x_max = 0.7

    def _tts_worker(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ SAPI ì—”ì§„ì„ ì´ˆê¸°í™”í•˜ê³  ì•ˆë‚´ë¥¼ ì²˜ë¦¬ (ê°€ì¥ í™•ì‹¤í•œ ìœˆë„ìš° ë°©ì‹)"""
        pythoncom.CoInitialize()
        speaker = win32com.client.Dispatch("SAPI.SpVoice")
        
        while True:
            # íì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ (í…ìŠ¤íŠ¸, ê°•ì œì¤‘ì§€ì—¬ë¶€)
            item = self.speech_queue.get()
            if item is None: break
            
            text, force_stop = item
            
            # ë®¤íŠ¸ ìƒíƒœë©´ ë¬´ì‹œ (ë‹¨, ê°•ì œ ì¢…ë£Œ ì•ˆë‚´ëŠ” ì˜ˆì™¸)
            if self.is_muted and not force_stop:
                self.speech_queue.task_done()
                continue

            # ì‹¤ì‹œê°„ ë³¼ë¥¨ ì ìš©
            speaker.Volume = self.volume

            # force_stopì´ Trueì´ë©´ í˜„ì¬ ë§í•˜ê³  ìˆëŠ” ê²ƒê³¼ ë°€ë ¤ìˆëŠ” íë¥¼ ëª¨ë‘ ë¬´ì‹œí•˜ê³  ì¦‰ì‹œ ë§í•¨
            # SAPI Flag: 2 (SVSFPurgeBeforeSpeak)
            flags = 2 if force_stop else 0
            
            print(f"[TTS ë°œí™” ì‹œì‘] {text} (ê°•ì œì¢…ë£Œ: {force_stop})")
            try:
                speaker.Speak(text, flags)
            except Exception as e:
                print(f"[TTS ì˜¤ë¥˜] {e}")
            print(f"[TTS ë°œí™” ì™„ë£Œ] {text}")
            self.speech_queue.task_done()

    def _stt_worker(self):
        """ë§ˆì´í¬ ì†Œë¦¬ë¥¼ ë“£ê³  ëª…ë ¹ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ìŠ¤ë ˆë“œ"""
        model = Model(self.model_path)
        rec = KaldiRecognizer(model, 16000)
        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=8000)
        stream.start_stream()

        print("ğŸ™ï¸ ìŒì„± ì¸ì‹ ì¤€ë¹„ ì™„ë£Œ. ëª…ë ¹ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")

        while True:
            data = stream.read(4000, exception_on_overflow=False)
            if rec.AcceptWaveform(data):
                result = json.loads(rec.Result())
                text = result.get("text", "").replace(" ", "")
                if not text: continue

                print(f"ğŸ‘‚ ìŒì„± ì¸ì‹ ê²°ê³¼: {text}")
                self.handle_command(text)

    def handle_command(self, text):
        """ìŒì„± ì¸ì‹ì„ í†µí•´ ë“¤ì–´ì˜¨ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ëª…ë ¹ ìˆ˜í–‰"""
        # ëª…ë ¹ì–´ íŒë³„ (ê³µë°± ì œê±° í›„ ë¹„êµ)
        text = text.replace(" ", "")
        
        if "ì¢…ë£Œ" in text:
            self.speak("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.", force_stop=True)
            self.running = False
        elif "ë‹¤ì‹œì‹œì‘" in text or "ë‹¤ì‹œì‹¤í–‰" in text:
            self.speak("ì‹œìŠ¤í…œì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.", force_stop=True)
        elif "ë³¼ë¥¨ì˜¬ë ¤" in text:
            self.volume = min(100, self.volume + 20)
            self.speak(f"ë³¼ë¥¨ì„ ì˜¬ë ¸ìŠµë‹ˆë‹¤. í˜„ì¬ ë³¼ë¥¨ {self.volume}")
        elif "ë³¼ë¥¨ë‚´ë ¤" in text:
            self.volume = max(0, self.volume - 20)
            self.speak(f"ë³¼ë¥¨ì„ ë‚´ë ¸ìŠµë‹ˆë‹¤. í˜„ì¬ ë³¼ë¥¨ {self.volume}")
        elif "ì¡°ìš©íˆí•´" in text or "ì •ì§€í•´" in text:
            self.is_muted = True
            self.speak("ìŒì„± ì•ˆë‚´ë¥¼ ì¼ì‹œ ì •ì§€í•©ë‹ˆë‹¤.", force_stop=True)
        elif "ë§í•´ì¤˜" in text or "ë‹¤ì‹œë§í•´" in text:
            self.is_muted = False
            self.speak("ìŒì„± ì•ˆë‚´ë¥¼ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")

    def speak(self, text, force_stop=False):
        """ì•ˆë‚´ ë¬¸êµ¬ë¥¼ íì— ì¶”ê°€ (ë¹„ë™ê¸°)"""
        if force_stop:
            # ê¸°ì¡´ íì— ìŒ“ì¸ ëª¨ë“  ë©”ì‹œì§€ ë¬´ì‹œí•˜ë„ë¡ í ë¹„ìš°ê¸° ì‹œë„
            while not self.speech_queue.empty():
                try:
                    self.speech_queue.get_nowait()
                    self.speech_queue.task_done()
                except:
                    break
        self.speech_queue.put((text, force_stop))

    def stage2_yolo_optimized(self, frame):
        results = self.yolo_model(frame, imgsz=320, verbose=False) 
        objects = []
        for r in results:
            boxes = r.boxes
            for box in boxes:
                b = box.xyxy[0].cpu().numpy().astype(int)
                cls_id = int(box.cls[0])
                model_label = self.yolo_model.names[cls_id]
                ko_label = self.class_names_ko.get(model_label, model_label)
                objects.append({'box': b, 'label': ko_label})
        return objects

    def stage3_depth_optimized(self, frame):
        small_frame = cv2.resize(frame, (256, 256)) 
        img = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)
        input_batch = self.transform(img).to(self.device)

        with torch.no_grad():
            prediction = self.midas(input_batch)
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),
                size=frame.shape[:2],
                mode="bicubic",
                align_corners=False,
            ).squeeze()

        depth_map = prediction.cpu().numpy()
        depth_min, depth_max = depth_map.min(), depth_map.max()
        depth_norm = (255 * (depth_map - depth_min) / (depth_max - depth_min + 1e-5)).astype(np.uint8)
        depth_color = cv2.applyColorMap(depth_norm, cv2.COLORMAP_MAGMA)
        
        return depth_map, depth_color

    def raw_to_meters(self, raw_val):
        if raw_val <= 0: return float('inf')
        meters = self.K_DEPTH / (raw_val + 1e-5)
        return meters

    def run(self):
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        window_name_main = "MVP Test - Color (YOLO)"
        window_name_depth = "MVP Test - Depth (MiDaS)"
        cv2.namedWindow(window_name_main)
        cv2.namedWindow(window_name_depth)

        print("\n=== ìŒì„± ì•ˆë‚´(TTS)ê°€ ìµœì í™”ëœ MVP íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        # ì‹œì‘ ì‹œ ì•ˆë‚´ ìŒì„± ì¶”ê°€ (ì›¹ì—ì„œ ë‹¤ì‹œ ì‹œì‘í•  ë•Œë„ ë‚˜ì˜´)
        self.speak("ë³´ì¡° ì‹œìŠ¤í…œ ì•ˆë‚´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", force_stop=True)

        self.running = True
        last_log_time = 0
        log_interval = 6.0 

        while self.running:
            ret, frame = cap.read()
            if not ret: break
            
            self.frame_count += 1
            current_time = time.time()
            
            # --- íŒŒì´í”„ë¼ì¸ ì—°ì‚° ---
            if self.frame_count % self.frame_skip == 1 or self.last_depth_map is None:
                self.last_objects = self.stage2_yolo_optimized(frame)
                self.last_depth_map, self.last_depth_viz = self.stage3_depth_optimized(frame)
            
            display_frame = frame.copy()
            should_log = (current_time - last_log_time) >= log_interval

            # --- ROI í•„í„°ë§ ë° ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ì„ íƒ ---
            h, w = frame.shape[:2]
            roi_left = int(w * self.roi_x_min)
            roi_right = int(w * self.roi_x_max)
            
            closest_obj = None
            min_meters = float('inf')

            for obj in self.last_objects:
                b = obj['box']
                cx = (b[0] + b[2]) // 2
                cy = int(b[3] * 0.9)
                
                # ROI ë‚´ë¶€ì— ì¤‘ì‹¬ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                if roi_left <= cx <= roi_right:
                    h_d, w_d = self.last_depth_map.shape
                    cx_d, cy_d = max(0, min(cx, w_d-1)), max(0, min(cy, h_d-1))
                    
                    raw_val = self.last_depth_map[cy_d, cx_d]
                    meters = self.raw_to_meters(raw_val)
                    
                    # ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ê°±ì‹ 
                    if meters < min_meters:
                        min_meters = meters
                        closest_obj = {
                            'label': obj['label'],
                            'box': b,
                            'meters': meters,
                            'cx': cx
                        }

            # --- ì‹œê°í™” ë° ì•ˆë‚´ ---
            # ROI ê°€ì´ë“œ ë¼ì¸ í‘œì‹œ
            cv2.line(display_frame, (roi_left, 0), (roi_left, h), (0, 0, 255), 2)
            cv2.line(display_frame, (roi_right, 0), (roi_right, h), (0, 0, 255), 2)

            current_labels = set()
            if closest_obj and min_meters < 10.0:
                b = closest_obj['box']
                label_name = closest_obj['label']
                meters = closest_obj['meters']
                current_labels.add(label_name)

                # ì‹œê°í™” (ì„ íƒëœ ë¬¼ì²´ë§Œ ê°•ì¡°)
                cv2.rectangle(display_frame, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)
                cv2.putText(display_frame, f"TARGET: {label_name} {meters:.1f}m", (b[0], b[1]-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

                # --- ìŒì„± ì•ˆë‚´ ë¡œì§ ---
                if label_name not in self.announced_objects:
                    self.speak(f"ì „ë°©ì— {label_name}ê°€ ìˆìŠµë‹ˆë‹¤. ê±°ë¦¬ëŠ” {meters:.1f} ë¯¸í„°ì…ë‹ˆë‹¤.")
                    self.announced_objects[label_name] = current_time

                if should_log:
                    print(f"[ë³´í–‰ ë³´ì¡°] ì¥ì• ë¬¼ ê°ì§€: {label_name} | ê±°ë¦¬: {meters:.1f}m")

            # ì•ˆë‚´ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì˜¤ë«ë™ì•ˆ ì•ˆ ë³´ì¸ ì‚¬ë¬¼ì€ ëª©ë¡ì—ì„œ ì œê±°)
            for label in list(self.announced_objects.keys()):
                if label not in current_labels:
                    if current_time - self.announced_objects[label] > self.announce_timeout:
                        del self.announced_objects[label]

            if should_log:
                last_log_time = current_time

            # í™”ë©´ í‘œì‹œ

            # í™”ë©´ í‘œì‹œ
            # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš©ìœ¼ë¡œ í˜„ì¬ í”„ë ˆì„ ì €ì¥
            with self.frame_lock:
                self.last_web_frame = display_frame.copy()

            cv2.imshow(window_name_main, display_frame)
            if self.last_depth_viz is not None:
                cv2.imshow(window_name_depth, self.last_depth_viz)
            
            # ì¢…ë£Œ ë¡œì§
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27: # Që‚˜ ESC
                break
            
            # ì°½ì´ ë‹«í˜”ëŠ”ì§€ í™•ì¸
            if cv2.getWindowProperty(window_name_main, cv2.WND_PROP_VISIBLE) < 1:
                break

        cap.release()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    pipeline = MVPTestPipeline()
    pipeline.run()
